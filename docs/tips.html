<!DOCTYPE html>
<html>
<head>
<title>tips.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/*

github.com style (c) Vasily Polovnyov <vast@whiteants.net>

*/

.hljs {
  display: block;
  overflow-x: auto;
  padding: 0.5em;
  color: #333;
  background: #f8f8f8;
}

.hljs-comment,
.hljs-quote {
  color: #998;
  font-style: italic;
}

.hljs-keyword,
.hljs-selector-tag,
.hljs-subst {
  color: #333;
  font-weight: bold;
}

.hljs-number,
.hljs-literal,
.hljs-variable,
.hljs-template-variable,
.hljs-tag .hljs-attr {
  color: #008080;
}

.hljs-string,
.hljs-doctag {
  color: #d14;
}

.hljs-title,
.hljs-section,
.hljs-selector-id {
  color: #900;
  font-weight: bold;
}

.hljs-subst {
  font-weight: normal;
}

.hljs-type,
.hljs-class .hljs-title {
  color: #458;
  font-weight: bold;
}

.hljs-tag,
.hljs-name,
.hljs-attribute {
  color: #000080;
  font-weight: normal;
}

.hljs-regexp,
.hljs-link {
  color: #009926;
}

.hljs-symbol,
.hljs-bullet {
  color: #990073;
}

.hljs-built_in,
.hljs-builtin-name {
  color: #0086b3;
}

.hljs-meta {
  color: #999;
  font-weight: bold;
}

.hljs-deletion {
  background: #fdd;
}

.hljs-addition {
  background: #dfd;
}

.hljs-emphasis {
  font-style: italic;
}

.hljs-strong {
  font-weight: bold;
}

</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="howard-tips">HOWARD Tips</h1>
<details><summary>How to hive partitioning into Parquet a VCF format file?</summary>
<p>In order to create a database from a VCF file, process partitioning highly speed up futher annotation process. Simply use HOWARD Convert tool with <code>--parquet_partitions</code> option. Format of input and output files can be any avaialbe format (e.g. Parquet, VCF, TSV).</p>
<p>This process is not ressource intensive, but can take a while for huge databases. However, using <code>--explode_infos</code> option require much more memory for huge databases.</p>
<pre class="hljs"><code><div>INPUT=~/howard/databases/dbsnp/current/hg19/b156/dbsnp.parquet
OUTPUT=~/howard/databases/dbsnp/current/hg19/b156/dbsnp.partition.parquet
PARTITIONS=<span class="hljs-string">"#CHROM"</span> <span class="hljs-comment"># "#CHROM", "#CHROM,REF,ALT" (for SNV file only) </span>
howard convert \
   --input=<span class="hljs-variable">$INPUT</span> \
   --output=<span class="hljs-variable">$OUTPUT</span> \
   --parquet_partitions=<span class="hljs-string">"#CHROM"</span> \
   --threads=8
</div></code></pre>
</details>
<details><summary>How to hive partitioning into Parquet a huge VCF format file with all annotations exploded into columns?</summary>
<p>Due to memory usage with duckDB, huge VCF file convertion can fail. This tip describe hive partitioning of huge VCF files into Parquet, to prevent memory resource crash.</p>
<p>The following bash script is spliting VCF by &quot;#CHROM&quot; and chunk VCF files with a defined size (&quot;CHUNK_SIZE&quot;). Depending on input VCF file, the number of chunk VCF files will be determined by the content (usually number of annotation within the VCF file). Moreover, Parquet files can also be chunked (&quot;CHUNK_SIZE_PARQUET&quot;). Default options This will ensure a memory usage around 4-5Gb.</p>
<p>Moreover, an additional partitioning can be applied, on one or more specific VCF column or INFO annotation: &quot;None&quot; for no partitioning; &quot;REF,ALT&quot; for VCF only with SNV (e.g. dbSNP); &quot;CLINSIG&quot; for ClinVar database (values such as &quot;Pathogenic, Moslty-Pathogenic&quot;...). Note that partitioning only works for small values (a value lenght will create a too long partition folder), and for not too many values for a column (partition fragments is maximum of 1024). This additional partition can take a while.</p>
<p>In order to create a high-performance database for HOWARD, INFO annotations can be exploded in columns. This option is slower, but generate a Parquet database that will be used by picking needed columns for annotation. Annotations to explode can be chosen with more options.</p>
<pre class="hljs"><code><div><span class="hljs-comment"># Files</span>
VCF=/tmp/my.vcf.gz                  <span class="hljs-comment"># Input VCF file</span>
PARQUET=/tmp/my.partition.parquet   <span class="hljs-comment"># Output Partitioned Parquet folder</span>

<span class="hljs-comment"># Tools</span>
BCFTOOLS=bcftools   <span class="hljs-comment"># BCFTools</span>
BGZIP=bgzip         <span class="hljs-comment"># BGZip</span>
HOWARD=howard       <span class="hljs-comment"># HOWARD</span>
TABIX=tabix         <span class="hljs-comment"># Tabix</span>

<span class="hljs-comment"># Threads</span>
THREADS=12          <span class="hljs-comment"># Number of threads</span>

<span class="hljs-comment"># Param</span>
CHUNK_SIZE=1000000000               <span class="hljs-comment"># 1000000000 for VCF chunk size around 200Mb</span>
CHUNK_SIZE_PARQUET=10000000         <span class="hljs-comment"># 10000000 for parquet chunk size around 200Mb</span>
PARQUET_PARTITIONS=<span class="hljs-string">"None"</span>           <span class="hljs-comment"># "None" for no more partition, "REF,ALT" (SNV VCF) or "REF" or "CLNSIG"...</span>
CONVERT_OPTIONS=<span class="hljs-string">" --explode_infos "</span> <span class="hljs-comment"># Explode INFO annotations into columns</span>

<span class="hljs-comment"># Create output folder</span>
rm -r <span class="hljs-variable">$PARQUET</span>
mkdir -p <span class="hljs-variable">$PARQUET</span>

<span class="hljs-comment"># Extract header</span>
<span class="hljs-variable">$BCFTOOLS</span> view -h <span class="hljs-variable">$VCF</span> --threads <span class="hljs-variable">$THREADS</span> &gt; <span class="hljs-variable">$PARQUET</span>/header.vcf

<span class="hljs-comment"># VCF indexing (if necessary)</span>
<span class="hljs-keyword">if</span> [ ! -e <span class="hljs-variable">$VCF</span>.tbi ]; <span class="hljs-keyword">then</span>
    <span class="hljs-variable">$TABIX</span> <span class="hljs-variable">$VCF</span>
<span class="hljs-keyword">fi</span>;

<span class="hljs-comment"># For each chromosome</span>
<span class="hljs-keyword">for</span> chr <span class="hljs-keyword">in</span> $(<span class="hljs-variable">$TABIX</span> -l <span class="hljs-variable">$VCF</span> | cut -f1); <span class="hljs-keyword">do</span>

    <span class="hljs-keyword">if</span> [ <span class="hljs-string">"<span class="hljs-variable">$chr</span>"</span> != <span class="hljs-string">"None"</span> ]; <span class="hljs-keyword">then</span>
        <span class="hljs-built_in">echo</span> <span class="hljs-string">"# Chromosome '<span class="hljs-variable">$chr</span>'"</span>

        <span class="hljs-comment"># Create chromosome folder</span>
        mkdir -p <span class="hljs-variable">$PARQUET</span>/<span class="hljs-comment">#CHROM=$chr;</span>

        <span class="hljs-built_in">echo</span> <span class="hljs-string">"# Chromosome '<span class="hljs-variable">$chr</span>' - BCFTools filter and split file..."</span>
        <span class="hljs-variable">$BCFTOOLS</span> filter <span class="hljs-variable">$VCF</span> -r <span class="hljs-variable">$chr</span> --threads <span class="hljs-variable">$THREADS</span> | <span class="hljs-variable">$BCFTOOLS</span> view -H --threads <span class="hljs-variable">$THREADS</span> | split -a 10 -C <span class="hljs-variable">$CHUNK_SIZE</span> - <span class="hljs-variable">$PARQUET</span>/<span class="hljs-comment">#CHROM=$chr/ --filter="$BGZIP -l1 --threads=$THREADS &gt; \$FILE.gz";</span>
        nb_chunk_files=$(ls <span class="hljs-variable">$PARQUET</span>/<span class="hljs-comment">#CHROM=$chr/*.gz | wc -l)</span>

        <span class="hljs-comment"># Convert chunk VCF to Parquet</span>
        i_chunk_files=0
        <span class="hljs-keyword">for</span> file <span class="hljs-keyword">in</span> <span class="hljs-variable">$PARQUET</span>/<span class="hljs-comment">#CHROM=$chr/*.gz; do</span>
            
            <span class="hljs-comment"># Chunk file to TSV and header</span>
            ((i_chunk_files++))
            chunk=$(basename <span class="hljs-variable">$file</span> | sed s/.gz$//gi)
            <span class="hljs-built_in">echo</span> <span class="hljs-string">"# Chromosome '<span class="hljs-variable">$chr</span>' - Convert VCF to Parquet '<span class="hljs-variable">$chunk</span>' (<span class="hljs-variable">$PARQUET_PARTITIONS</span>) [<span class="hljs-variable">$i_chunk_files</span>/<span class="hljs-variable">$nb_chunk_files</span>]..."</span>
            mv <span class="hljs-variable">$file</span> <span class="hljs-variable">$file</span>.tsv.gz;
            cp <span class="hljs-variable">$PARQUET</span>/header.vcf <span class="hljs-variable">$file</span>.tsv.gz.hdr;

            <span class="hljs-comment"># Convert with partitioning or not</span>
            <span class="hljs-variable">$HOWARD</span> convert --input=<span class="hljs-variable">$file</span>.tsv.gz --output=<span class="hljs-variable">$file</span>.parquet <span class="hljs-variable">$CONVERT_OPTIONS</span> --threads=<span class="hljs-variable">$THREADS</span> --parquet_partitions=<span class="hljs-string">"<span class="hljs-variable">$PARQUET_PARTITIONS</span>"</span> --verbosity=ERROR --chunk_size=<span class="hljs-variable">$CHUNK_SIZE_PARQUET</span>

            <span class="hljs-comment"># Redirect partitioninf folder if any</span>
            <span class="hljs-keyword">if</span> [ <span class="hljs-string">"<span class="hljs-variable">$PARQUET_PARTITIONS</span>"</span> != <span class="hljs-string">""</span> ]; <span class="hljs-keyword">then</span>
                rsync -a <span class="hljs-variable">$file</span>.parquet/ $(dirname <span class="hljs-variable">$file</span>)/
                rm -r <span class="hljs-variable">$file</span>.parquet*
            <span class="hljs-keyword">fi</span>;

            <span class="hljs-comment"># Clean</span>
            rm -f <span class="hljs-variable">$file</span>.tsv*

        <span class="hljs-keyword">done</span>;
    <span class="hljs-keyword">fi</span>;
<span class="hljs-keyword">done</span>;

<span class="hljs-comment"># Create header</span>
cp <span class="hljs-variable">$PARQUET</span>/header.vcf <span class="hljs-variable">$PARQUET</span>.hdr

<span class="hljs-comment"># Show partitioned Parquet folder</span>
tree -h <span class="hljs-variable">$PARQUET</span>

</div></code></pre>
</details>
<details><summary>How to aggregate all INFO annotations from multiple Parquet databases into one INFO field?</summary>
<p>In order to merge all annotations in INFO column of multiple databases, use a SQL query on the list of Parquet databases, and use <code>STRING_AGG</code> duckDB function to aggretate values.
This will probably work only for small databases.</p>
<pre class="hljs"><code><div>howard query \
   --explode_infos \
   --explode_infos_prefix=<span class="hljs-string">'INFO/'</span> \
   --query=<span class="hljs-string">"SELECT \"#CHROM\", POS, REF, ALT, STRING_AGG(INFO, ';') AS INFO \
            FROM parquet_scan('tests/databases/annotations/current/hg19/*.parquet', union_by_name = true) \
            GROUP BY \"#CHROM\", POS, REF, ALT"</span> \
   --output=/tmp/full_annotation.tsv
   
head -n2 /tmp/full_annotation.tsv
</div></code></pre>
<pre class="hljs"><code><div>#CHROM  POS     REF     ALT     INFO
chr1    69093   G       T       MCAP13=0.001453;REVEL=0.117;SIFT_score=0.082;SIFT_converted_rankscore=0.333;SIFT_pred=T;SIFT4G_score=0.097;SIFT4G_converted_rankscore=0.392;SIFT4G_pred=T;Polyphen2_HDIV_score=0.0;Polyphen2_HDIV_rankscore=0.029;Polyphen2_HDIV_pred=B;Polyphen2_HVAR_score=0.0;Polyphen2_HVAR_rankscore=0.014;Polyphen2_HVAR_pred=B;LRT_score=0.589;LRT_converted_rankscore=0.056;LRT_pred=N;MutationTaster_score=0.635;MutationTaster_converted_rankscore=0.328;MutationTaster_pred=D;MutationAssessor_score=.;MutationAssessor_rankscore=.;MutationAssessor_pred=.;FATHMM_score=6.74;FATHMM_converted_rankscore=0.005;FATHMM_pred=T;PROVEAN_score=0.27;PROVEAN_converted_rankscore=0.043;PROVEAN_pred=N;VEST4_score=0.12;VEST4_rankscore=0.111;MetaSVM_score=-1.003;MetaSVM_rankscore=0.291;MetaSVM_pred=T;MetaLR_score=0.002;MetaLR_rankscore=0.005;MetaLR_pred=T;MetaRNN_score=0.452;MetaRNN_rankscore=0.666;MetaRNN_pred=T;M_CAP_score=0.001;M_CAP_rankscore=0.022;M_CAP_pred=T;REVEL_score=0.117;REVEL_rankscore=0.332;MutPred_score=0.835;MutPred_rankscore=0.943;MVP_score=0.240;MVP_rankscore=0.236;MPC_score=.;MPC_rankscore=.;PrimateAI_score=.;PrimateAI_rankscore=.;PrimateAI_pred=.;DEOGEN2_score=0.008;DEOGEN2_rankscore=0.072;DEOGEN2_pred=T;BayesDel_addAF_score=-0.359;BayesDel_addAF_rankscore=0.042;BayesDel_addAF_pred=T;BayesDel_noAF_score=-0.754;BayesDel_noAF_rankscore=0.033;BayesDel_noAF_pred=T;ClinPred_score=0.090;ClinPred_rankscore=0.112;ClinPred_pred=T;LIST_S2_score=0.065;LIST_S2_rankscore=0.651;LIST_S2_pred=T;Aloft_pred=.,.,;Aloft_Confidence=.,.,;CADD_raw=0.013;CADD_raw_rankscore=0.049;CADD_phred=2.83;DANN_score=0.602;DANN_rankscore=0.064;fathmm_MKL_coding_score=0.505;fathmm_MKL_coding_rankscore=0.287;fathmm_MKL_coding_pred=D;fathmm_XF_coding_score=0.012;fathmm_XF_coding_rankscore=0.001;fathmm_XF_coding_pred=N;Eigen_raw_coding=-0.958;Eigen_raw_coding_rankscore=0.095;Eigen_PC_raw_coding=-0.968;Eigen_PC_raw_coding_rankscore=0.105;GenoCanyon_score=0;GenoCanyon_rankscore=0.012;integrated_fitCons_score=0.487;integrated_fitCons_rankscore=0.14;integrated_confidence_value=0;LINSIGHT=.;LINSIGHT_rankscore=.;GERP___NR=2.31;GERP___RS=1.36;GERP___RS_rankscore=0.211;phyloP100way_vertebrate=1.139;phyloP100way_vertebrate_rankscore=0.311;phyloP30way_mammalian=0.113;phyloP30way_mammalian_rankscore=0.17;phastCons100way_vertebrate=0.841;phastCons100way_vertebrate_rankscore=0.303;phastCons30way_mammalian=0.552;phastCons30way_mammalian_rankscore=0.281;SiPhy_29way_logOdds=6.575;SiPhy_29way_logOdds_rankscore=0.218;Interpro_domain=.,.;GTEx_V8_gene=.;GTEx_V8_tissue=.;InterVar_automated=Uncertain_significance;PVS1=0;PS1=0;PS2=0;PS3=0;PS4=0;PM1=0;PM2=1;PM3=0;PM4=0;PM5=0;PM6=0;PP1=0;PP2=0;PP3=0;PP4=0;PP5=0;BA1=0;BS1=0;BS2=0;BS3=0;BS4=0;BP1=0;BP2=0;BP3=0;BP4=1;BP5=0;BP6=0;BP7=0
</div></code></pre>
</details>
<details><summary>How to explore genetics variations from VCF files?</summary>
<p><a href="https://cutevariant.labsquare.org/">CuteVariant: A standalone and free application to explore genetics variations from VCF files</a></p>
<p>Cutevariant is a cross-plateform application dedicated to maniupulate and filter variation from annotated VCF file. When you create a project, data are imported into an sqlite database that cutevariant queries according your needs. Presently, SnpEff and VEP annotations are supported. Once your project is created, you can query variant using different gui controller or directly using the VQL language. This Domain Specific Language is specially designed for cutevariant and try to keep the same syntax than SQL for an easy use.</p>
<p>Published in <a href="https://academic.oup.com/bioinformaticsadvances/article/2/1/vbab028/6440032?login=true">Bioinformatics Advanced - Cutevariant: a standalone GUI-based desktop application to explore genetic variations from an annotated VCF file</a></p>
<p>Documentation available on <a href="https://cutevariant.labsquare.org/">cutevariant.labsquare.org</a> and <a href="(https://github.com/labsquare/cutevariant)">GitHub</a></p>
<p><img src="https://raw.githubusercontent.com/labsquare/cutevariant/master/screencast.gif" alt="CuteVariant" title="HOWARD Graphical User Interface"></p>
</details>

</body>
</html>
